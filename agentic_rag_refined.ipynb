{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0531212f",
   "metadata": {},
   "source": [
    "# Agentic RAG (LangGraph + Local GGUF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a9bc29",
   "metadata": {},
   "source": [
    "## Tartalom\n",
    "1. Környezet és konfiguráció\n",
    "2. Adatbetöltés és feldarabolás (PDF → chunkok)\n",
    "3. Beágyazás és indexelés (FAISS)\n",
    "4. Agent komponensek (Planner, Retriever, Reranker, Synthesize, Critic)\n",
    "5. LangGraph: gráf felépítése és vezérlő logika\n",
    "6. Példák és demó\n",
    "7. Teljesítménymérés és bottleneck elemzés\n",
    "8. Gyors tesztek (smoke tests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a506531",
   "metadata": {},
   "source": [
    "## 1) Környezet és konfiguráció"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a76302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config betöltve. PDF dir: data/pdfs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Általános importok\n",
    "import json, time, random, yaml, re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# PDF\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Embedding & Index\n",
    "import faiss\n",
    "\n",
    "# Rerank / Embeddings (fallback-okkal)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    _CROSSENCODER_AVAILABLE = True\n",
    "except Exception:\n",
    "    _CROSSENCODER_AVAILABLE = False\n",
    "\n",
    "# LLM (local,)\n",
    "from typing import Callable\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    _LLAMA_AVAILABLE = True\n",
    "except Exception:\n",
    "    _LLAMA_AVAILABLE = False\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Konfig betöltése\n",
    "CFG_PATH = Path(\"config.yaml\")\n",
    "with open(CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "random.seed(CFG.get(\"seed\", 42))\n",
    "np.random.seed(CFG.get(\"seed\", 42))\n",
    "\n",
    "DATA_DIR = Path(CFG[\"paths\"][\"data_dir\"])\n",
    "PDF_DIR = Path(CFG[\"data\"][\"pdf_dir\"])\n",
    "INDEX_DIR = Path(CFG[\"paths\"][\"index_dir\"])\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_SIZE = CFG[\"chunking\"][\"chunk_size\"]\n",
    "CHUNK_OVERLAP = CFG[\"chunking\"][\"chunk_overlap\"]\n",
    "TOP_K = CFG[\"retrieval\"][\"top_k\"]\n",
    "RERANK_TOP_K = CFG[\"retrieval\"][\"rerank_top_k\"]\n",
    "\n",
    "GGUF_MODEL_PATH = CFG[\"models\"][\"gguf_model_path\"]\n",
    "N_CTX = CFG[\"models\"][\"n_ctx\"]\n",
    "N_THREADS = CFG[\"models\"][\"n_threads\"]\n",
    "TEMPERATURE = CFG[\"models\"][\"temperature\"]\n",
    "TOP_P = CFG[\"models\"][\"top_p\"]\n",
    "\n",
    "print(\"Config betöltve. PDF dir:\", PDF_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e1772",
   "metadata": {},
   "source": [
    "## 2) Adatbetöltés és feldarabolás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6d40cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betöltött dokumentumok: 1\n",
      "Összes chunk: 61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_pdfs(pdf_dir: Path, pattern: str = \"*.pdf\") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    PDF-ek betöltése. Visszaad: list[(doc_id, full_text)]\n",
    "    doc_id: relatív fájlnév\n",
    "    \"\"\"\n",
    "    pdf_paths = list(pdf_dir.glob(pattern))\n",
    "    docs = []\n",
    "    for p in pdf_paths:\n",
    "        try:\n",
    "            reader = PdfReader(str(p))\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                try:\n",
    "                    text += page.extract_text() or \"\"\n",
    "                except Exception:\n",
    "                    pass\n",
    "            docs.append((p.name, text.strip()))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Nem sikerült olvasni: {p} | {e}\")\n",
    "    return docs\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 150) -> List[str]:\n",
    "    \"\"\"\n",
    "    Egyszerű karakter-alapú darabolás átfedéssel.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        i += max(1, chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "docs = load_pdfs(PDF_DIR, CFG[\"data\"][\"pdf_glob\"])\n",
    "print(f\"Betöltött dokumentumok: {len(docs)}\")\n",
    "\n",
    "# Chunkolás\n",
    "corpus = []\n",
    "for doc_id, full_text in docs:\n",
    "    chunks = chunk_text(full_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    for j, ch in enumerate(chunks):\n",
    "        corpus.append({\n",
    "            \"id\": f\"{doc_id}::chunk{j}\",\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": j,\n",
    "            \"text\": ch\n",
    "        })\n",
    "print(\"Összes chunk:\", len(corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575a9b6",
   "metadata": {},
   "source": [
    "## 3) Beágyazás és indexelés (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ecf3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index betöltve: 61 vektor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SafeEmbedder:\n",
    "    \"\"\"\n",
    "    Elsődleges: SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    Ha nem érhető el, fallback: determinisztikus bag-of-char n-gram vektor.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", dim: int = 384):\n",
    "        self.dim = dim\n",
    "        self.mode = \"st\"\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.dim = self.model.get_sentence_embedding_dimension()\n",
    "            self.mode = \"st\"\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] SentenceTransformer nem elérhető, fallback embedder indul.\", e)\n",
    "            self.mode = \"fallback\"\n",
    "            self.model = None\n",
    "            self.dim = 384\n",
    "\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        if self.mode == \"st\":\n",
    "            return np.array(self.model.encode(texts, normalize_embeddings=True), dtype=\"float32\")\n",
    "\n",
    "        vecs = np.zeros((len(texts), self.dim), dtype=\"float32\")\n",
    "        for i, t in enumerate(texts):\n",
    "            h = np.zeros(self.dim, dtype=\"float32\")\n",
    "            for j in range(len(t)-2):\n",
    "                ngram = t[j:j+3]\n",
    "                idx = (hash(ngram) % self.dim)\n",
    "                h[idx] += 1.0\n",
    "            norm = np.linalg.norm(h) + 1e-9\n",
    "            vecs[i] = h / norm\n",
    "        return vecs\n",
    "\n",
    "EMBEDDER = SafeEmbedder()\n",
    "\n",
    "def build_faiss_index(chunks: List[Dict[str, Any]], index_dir: Path):\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    if len(texts) == 0:\n",
    "        idx = faiss.IndexFlatIP(EMBEDDER.dim)\n",
    "        faiss.write_index(idx, str(index_dir / \"chunks.index\"))\n",
    "        with open(index_dir / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([], f, ensure_ascii=False)\n",
    "        return idx, []\n",
    "    vecs = EMBEDDER.encode(texts).astype(\"float32\")\n",
    "    index = faiss.IndexFlatIP(vecs.shape[1])\n",
    "    index.add(vecs)\n",
    "    faiss.write_index(index, str(index_dir / \"chunks.index\"))\n",
    "    with open(index_dir / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False)\n",
    "    return index, chunks\n",
    "\n",
    "def load_faiss_index(index_dir: Path):\n",
    "    try:\n",
    "        index = faiss.read_index(str(index_dir / \"chunks.index\"))\n",
    "        with open(index_dir / \"meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        return index, meta\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# Build or load\n",
    "index, meta = load_faiss_index(INDEX_DIR)\n",
    "if index is None:\n",
    "    if len(corpus) > 0:\n",
    "        index, meta = build_faiss_index(corpus, INDEX_DIR)\n",
    "        print(\"FAISS index létrehozva.\")\n",
    "    else:\n",
    "        index, meta = build_faiss_index([], INDEX_DIR)\n",
    "        print(\"[WARN] Nincsenek PDF-ek. Üres index jött létre a demóhoz.\")\n",
    "else:\n",
    "    print(\"FAISS index betöltve:\", getattr(index, 'ntotal', '?'), \"vektor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6177ed8",
   "metadata": {},
   "source": [
    "## 4) Agent komponensek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbb9e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M4) - 15343 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "............load_tensors: offloaded 0/33 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".......................................................................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4\n",
      "ggml_metal_init: picking default device: Apple M4\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "...............................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4\n",
      "ggml_metal_init: picking default device: Apple M4\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x12bc13dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x12bd1a950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x13e7d5e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x13e7daae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x12bc16ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x12bd192d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x12bc15530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x12bd18e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x13e8596b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x13e8590c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x12bd1aff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x13e7d6260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x13e859cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x13e85a920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x12bc12650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x13e85a310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x12bc10ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x12bc0f770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x12bd1c070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x12bc0c5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x13e7dc600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x13e7db5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x13e7dd340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x12bc0adf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x12bd1b8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x12bd1cec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x13e7dca90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x12bc07f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x13e7de1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x12bd1c6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x12bd1dbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x12bc067b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x13e7deae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: GPU name:   Apple M4\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 17179.89 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x12bc13dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x12bd1a950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x13e7d5e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x13e7daae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x12bc16ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x12bd192d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x12bc15530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x12bd18e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x13e8596b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x13e8590c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x12bd1aff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x13e7d6260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x13e859cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x13e85a920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x12bc12650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x13e85a310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x12bc10ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x12bc0f770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x12bd1c070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x12bc0c5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x13e7dc600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x13e7db5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x13e7dd340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x12bc0adf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x12bd1b8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x12bd1cec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x13e7dca90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x12bc07f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x13e7de1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x12bd1c6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x12bd1dbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x12bc067b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x13e7deae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x13e7df610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x12bc05040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x12bc04080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x13e7dfc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x12bc04340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e7e08d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x13e7e1370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x13e7e2910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x12bc04600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x12bc048c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x12b9ab540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x12b9ab800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x12b9abac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x12b9abd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x13e7e4e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x13e7e42e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e85afe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x13e7e6630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e7e5270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e7e6ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b9ac040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x12b933790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x13e7df610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x12bc05040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x12bc04080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x13e7dfc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x12bc04340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e7e08d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x13e7e1370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x13e7e2910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x12bc04600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x12bc048c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x12b9ab540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x12b9ab800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x12b9abac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x12b9abd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x13e7e4e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x13e7e42e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e85afe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x13e7e6630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e7e5270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e7e6ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b9ac040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x12b933790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x12b933a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c5065f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b933d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e912e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e9128d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c704080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x13e7e7d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12bd1e800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c82d0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e914060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12bd1a4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e911ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e85c690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c506e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e9132b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x12b933a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c5065f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b933d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e912e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e9128d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c704080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x13e7e7d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12bd1e800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c82d0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e914060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12bd1a4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e911ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e85c690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c506e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e9132b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12bd1ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e85b910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e85bfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12bd1fd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e7e7790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c706120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x12c706c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x12c705be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x12bd1e2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x13e7e86f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x12bd20ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x13e7e9980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x12bd21340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x12bd21b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x12bd205b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x12b939830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x13e85d7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x13e85ec10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x12b9380c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x12bd22bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x13e85f550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b936950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12bd22370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x12b9351e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x13e7e93c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x12bd23ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12bd24580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x12b932020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e7ea790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12bd1ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e85b910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e85bfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12bd1fd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e7e7790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c706120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x12c706c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x12c705be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x12bd1e2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x13e7e86f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x12bd20ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x13e7e9980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x12bd21340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x12bd21b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x12bd205b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x12b939830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x13e85d7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x13e85ec10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x12b9380c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x12bd22bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x13e85f550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b936950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12bd22370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x12b9351e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x13e7e93c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x12bd23ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12bd24580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x12b932020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e7ea790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x12bd249d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x12bd249d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12bd256b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e85fdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14be069f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c5076d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b9308b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c607230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c507bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e9155e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x12b92f140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e85e3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e861220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e8619b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e862510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12bd24d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e863060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e7eaf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e862c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12bd26140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e7ebcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e861e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b92d9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e7ec780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e7eca40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e7ecd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e863650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b92c260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e7ed710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12bd256b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e85fdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14be069f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c5076d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b9308b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c607230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c507bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e9155e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x12b92f140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e85e3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e861220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e8619b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e862510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12bd24d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e863060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e7eaf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e862c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12bd26140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e7ebcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e861e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b92d9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e7ec780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e7eca40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e7ecd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e863650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b92c260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e7ed710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e863910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b9290a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e864b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e864640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b929360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e8642d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x12bd27340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x12bd281c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x12b929620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x13e8654d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12bd28ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e863910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b9290a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e864b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e864640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b929360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e8642d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x12bd27340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x12bd281c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x12b929620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x13e8654d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12bd28ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e865d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b92aaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b927930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e867900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b9261c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e867bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12bd277c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e8688e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12bd291d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12bd29710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b924a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b9232e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b921b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b920400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e7ee500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c82bf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12bd299d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e7ede70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b9871e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b94c4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e7ef8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b99c4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e7ef3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b9fc5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e7efd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b962aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e7f00a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e869c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b98a5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e86a3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e86a690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e86a950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e865d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b92aaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b927930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e867900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b9261c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e867bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12bd277c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e8688e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12bd291d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12bd29710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b924a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b9232e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b921b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b920400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e7ee500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c82bf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12bd299d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e7ede70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b9871e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b94c4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e7ef8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b99c4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e7ef3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b9fc5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e7efd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b962aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e7f00a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e869c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b98a5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e86a3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e86a690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e86a950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e86ad10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12bd2a960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e7f0f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b9d0760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x12bd2b020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b996360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b996620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e7f2330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12bd2c730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e7f0a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e7f1a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e7f3400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e86ad10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12bd2a960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e7f0f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b9d0760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x12bd2b020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b996360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b996620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e7f2330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12bd2c730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e7f0a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e7f1a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e7f3400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b9968e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12bd2b2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12bd2d900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b940570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12bd2e200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b9f2020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e7f45d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b93bd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12bd2e970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b981320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b9bdaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12bd2bdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12bd2edf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14be0a810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x12bd2f680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x12bd30330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b959810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b959ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e7f4890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b959d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12bd31880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12bd30800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e7f4c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12bd31340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e7f5be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12bd31c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12bd333c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12bd33ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e86bd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e7f64b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x12bd34430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x12bd32cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x12bd357b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x13e86c280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b9968e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12bd2b2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12bd2d900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b940570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12bd2e200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b9f2020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e7f45d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b93bd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12bd2e970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b981320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b9bdaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12bd2bdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12bd2edf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14be0a810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x12bd2f680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x12bd30330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b959810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b959ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e7f4890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b959d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12bd31880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12bd30800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e7f4c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12bd31340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e7f5be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12bd31c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12bd333c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12bd33ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e86bd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e7f64b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x12bd34430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x12bd32cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x12bd357b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x13e86c280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x13e7f75a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x12c5056a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x12bd36420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x13e86dc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x12bd36d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x12b9e87a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x12b99ea20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x12b96e0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x13e7f75a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x12c5056a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x12bd36420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x13e86dc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x12bd36d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x12b9e87a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x12b99ea20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x12b96e0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x13e86e580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x12b9d4020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x12bd37600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x13e86ee40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x13e86f780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x13e86c6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x12b94b130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x12b9e5fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x12b968c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x13e86fee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x13e8702e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x12bd35ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x12bd39190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x12bd39450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x12bd39710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x12b9b7960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x12bd39b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x13e871340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b958e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e870b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x12bd3a7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x12bd3a3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12bd3b020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b9de1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12bd3b770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b9a8860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x13e872180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x12b975c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b975ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e871c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x12b9761a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e872570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e872a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x13e86e580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x12b9d4020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x12bd37600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x13e86ee40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x13e86f780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x13e86c6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x12b94b130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x12b9e5fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x12b968c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x13e86fee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x13e8702e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x12bd35ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x12bd39190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x12bd39450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x12bd39710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x12b9b7960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x12bd39b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x13e871340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b958e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e870b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x12bd3a7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x12bd3a3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12bd3b020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b9de1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12bd3b770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b9a8860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x13e872180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x12b975c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b975ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e871c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x12b9761a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e872570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e872a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e872e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e874c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14be08380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c609060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c508250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e8745c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x12b9c2b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x12c605190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b9c2de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x14be17490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e872e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e874c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14be08380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c609060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c508250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e8745c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x12b9c2b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x12c605190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b9c2de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x14be17490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14be0d580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e9145a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c508f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c5095a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e7f6840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x12b9c30a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x14be17cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e875ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x12b9ca480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b957c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12bd3e540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b981c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b9d7ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12bd3cea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x13e876eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x12bd3c1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e7f8fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14be0d580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e9145a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c508f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c5095a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e7f6840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x12b9c30a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x14be17cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e875ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x12b9ca480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b957c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12bd3e540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b981c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b9d7ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12bd3cea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x13e876eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x12bd3c1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e7f8fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x12bd3ee30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12bd3fa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e8758b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12bd40030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12bd3f750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b9fe0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x13e877780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x12b9b0090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e7f9440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x13e7f9d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14be18160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e7fa1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c60a1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b970400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12bd41b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x13e9163e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x13e915e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b941730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x12b9aa8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12bd42840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12bd42ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e878a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b94ab30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e8763c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x13e7fb0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x12bd3ee30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12bd3fa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e8758b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12bd40030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12bd3f750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b9fe0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x13e877780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x12b9b0090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e7f9440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x13e7f9d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14be18160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e7fa1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c60a1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b970400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12bd41b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x13e9163e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x13e915e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b941730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x12b9aa8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12bd42840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12bd42ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e878a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b94ab30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e8763c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x13e7fb0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x13e8798c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b97c570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x13e7fb990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x13e87a170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x12b9cb800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x13e7fc4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x12b9b0c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x12b95af00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x13e7fcde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x12b9feca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x13e8783f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x13e87a4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x13e7fd6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x13e87adf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x12b994020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b9e1490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e87b6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e87cec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e7fe000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e87aaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e7fe8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x12b9f9e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x12b9717d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x13e87bcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x13e7fda40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x12b9bf940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x12b9531e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x13e87e430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x13e87f660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x12b9f0520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x12b990a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x12b9c6990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x13e87de00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b9669f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b99e470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf047e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b9f3a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12bd42300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b948730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x12b9489f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x12bd434b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x13e8798c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b97c570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x13e7fb990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x13e87a170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x12b9cb800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x13e7fc4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x12b9b0c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x12b95af00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x13e7fcde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x12b9feca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x13e8783f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x13e87a4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x13e7fd6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x13e87adf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x12b994020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b9e1490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e87b6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e87cec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e7fe000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e87aaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e7fe8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x12b9f9e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x12b9717d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x13e87bcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x13e7fda40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x12b9bf940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x12b9531e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x13e87e430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x13e87f660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x12b9f0520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x12b990a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x12b9c6990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x13e87de00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b9669f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b99e470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf047e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b9f3a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12bd42300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b948730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x12b9489f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x12bd434b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x12b948cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x12bd43a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x12b9979b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x12bd447c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x12b9e4270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x14bf04b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf05650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf05910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf05bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12bd44b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x12b948cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x12bd43a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x12b9979b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x12bd447c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x12b9e4270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x14bf04b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf05650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf05910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf05bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12bd44b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b95f430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf06b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf063c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b9efc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf06f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf07820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14bf084b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12bd45dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12b9989f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12b93de30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13e7f6e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15c82e9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12bd455e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12bd46ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12b9dbe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12b96e960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x14bf08880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x12b96ec20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x12bd43d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x14bf09280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x12c50a440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x12bd47550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x14bf0a660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x12bd480c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x13e87e970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x12bd483d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x12b96eee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x12b9dfe50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x13e881170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x14bf09740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x12b98af00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b9c59e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf09f40 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b95f430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf06b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf063c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b9efc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf06f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf07820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14bf084b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12bd45dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12b9989f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12b93de30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13e7f6e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15c82e9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12bd455e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12bd46ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12b9dbe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12b96e960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x14bf08880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x12b96ec20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x12bd43d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x14bf09280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x12c50a440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x12bd47550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x14bf0a660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x12bd480c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x13e87e970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x12bd483d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x12b96eee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x12b9dfe50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x13e881170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x14bf09740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x12b98af00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b9c59e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf09f40 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   512.00 MiB\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "llama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   300.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- LLM wrapper ---\n",
    "class DummyLLM:\n",
    "    \"\"\"\n",
    "    Fallback, ha nincs GGUF modell. Strukturált, magyarázó kimenetet ad.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, prompt: str, max_tokens: int = 512) -> str:\n",
    "        # nagyon egyszerű minta: kivonatolja a <<CONTEXT>> részt és válaszol\n",
    "        ctx = re.findall(r\"<<CONTEXT>>(.*?)<<END_CONTEXT>>\", prompt, flags=re.S|re.M)\n",
    "        ctx_text = (ctx[0] if ctx else \"\")[:500]\n",
    "        q = re.findall(r\"<<QUESTION>>(.*?)<<END_QUESTION>>\", prompt, flags=re.S|re.M)\n",
    "        question = (q[0] if q else \"\").strip()\n",
    "        return f\"Válasz (DummyLLM):\\n- Kérdés: {question}\\n- Kontextus-részlet: {ctx_text[:200]}...\\n- Megjegyzés: Helyi LLM nem elérhető, ezért sablon válasz készült.\"\n",
    "\n",
    "class LocalLLM:\n",
    "    \"\"\"\n",
    "    Llama.cpp alapú local modell.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path: str, n_ctx: int = 4096, n_threads: int = 4, temperature: float = 0.2, top_p: float = 0.9):\n",
    "        self.available = False\n",
    "        if _LLAMA_AVAILABLE and Path(model_path).exists():\n",
    "            self.llm = Llama(model_path=model_path, n_ctx=n_ctx, n_threads=n_threads, logits_all=False)\n",
    "            self.temperature = temperature\n",
    "            self.top_p = top_p\n",
    "            self.available = True\n",
    "        else:\n",
    "            print(\"[WARN] GGUF modell nem elérhető vagy llama-cpp nincs telepítve. DummyLLM indul.\")\n",
    "            self.llm = DummyLLM()\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 512) -> str:\n",
    "        if self.available:\n",
    "            out = self.llm(\n",
    "                prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=float({{}}) if False else 0.2,  # placeholder to avoid f-string\n",
    "                top_p=float({{}}) if False else 0.9,\n",
    "                stop=[\"</s>\"]\n",
    "            )\n",
    "            return out[\"choices\"][0][\"text\"]\n",
    "        else:\n",
    "            return self.llm(prompt, max_tokens=max_tokens)\n",
    "\n",
    "LLM = LocalLLM(GGUF_MODEL_PATH, n_ctx=N_CTX, n_threads=N_THREADS, temperature=TEMPERATURE, top_p=TOP_P)\n",
    "\n",
    "# --- Retrieval ---\n",
    "def retrieve(query: str, top_k: int = TOP_K) -> List[Dict[str, Any]]:\n",
    "    if getattr(index, 'ntotal', 0) == 0:\n",
    "        return []\n",
    "    qvec = EMBEDDER.encode([query]).astype(\"float32\")\n",
    "    D, I = index.search(qvec, top_k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx < 0 or idx >= len(meta): \n",
    "            continue\n",
    "        item = meta[idx].copy()\n",
    "        item[\"score\"] = float(score)\n",
    "        results.append(item)\n",
    "    return results\n",
    "\n",
    "# --- Rerank ---\n",
    "class SafeReranker:\n",
    "    def __init__(self):\n",
    "        self.mode = \"none\"\n",
    "        self.model = None\n",
    "        if _CROSSENCODER_AVAILABLE:\n",
    "            try:\n",
    "                self.model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "                self.mode = \"cross\"\n",
    "            except Exception as e:\n",
    "                self.mode = \"none\"\n",
    "\n",
    "    def rerank(self, query: str, docs: List[Dict[str, Any]], top_k: int = 4) -> List[Dict[str, Any]]:\n",
    "        if not docs:\n",
    "            return []\n",
    "        if self.mode == \"cross\" and self.model is not None:\n",
    "            pairs = [(query, d[\"text\"]) for d in docs]\n",
    "            scores = self.model.predict(pairs)\n",
    "            for d, s in zip(docs, scores):\n",
    "                d[\"rerank_score\"] = float(s)\n",
    "            docs = sorted(docs, key=lambda x: x.get(\"rerank_score\", 0.0), reverse=True)\n",
    "        return docs[:top_k]\n",
    "\n",
    "RERANKER = SafeReranker()\n",
    "\n",
    "# --- Prompt template ---\n",
    "BASE_SYSTEM = \"Te egy magyar nyelvű, forrás-alapú asszisztens vagy. Ha nincs releváns dokumentum, jelezd. Adj rövid, jól értelmezhető választ.\"\n",
    "\n",
    "def build_prompt(question: str, context_docs: List[Dict[str, Any]]) -> str:\n",
    "    ctx = \"\\n\\n\".join([f\"[{i+1}] {d['text']}\" for i, d in enumerate(context_docs)])\n",
    "    prompt = f\"\"\"{BASE_SYSTEM}\n",
    "\n",
    "<<CONTEXT>>\n",
    "{ctx}\n",
    "<<END_CONTEXT>>\n",
    "\n",
    "<<QUESTION>>\n",
    "{question}\n",
    "<<END_QUESTION>>\n",
    "\n",
    "Készíts rövid, pontokba szedett választ. Hivatkozz a sorszámokra (pl. [1], [2]) ahol indokolt.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# --- Simple planner ---\n",
    "def naive_plan(question: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Heurisztikus bontás: kötőszavak, felsorolások, 'és', 'majd', '?', stb.\n",
    "    \"\"\"\n",
    "    q = question.strip()\n",
    "    parts = re.split(r\"\\bés\\b|,|;|\\?|\\bmajd\\b\", q, flags=re.I)\n",
    "    parts = [p.strip() for p in parts if p.strip()]\n",
    "    if len(parts) <= 1:\n",
    "        parts = re.split(r\"[.!?]\", q)\n",
    "        parts = [p.strip() for p in parts if p.strip()]\n",
    "    return parts if parts else [q]\n",
    "\n",
    "# --- Critic ---\n",
    "def simple_groundedness_score(answer: str, docs: List[Dict[str, Any]]) -> float:\n",
    "    if not answer or not docs:\n",
    "        return 0.0\n",
    "    ctx = \" \".join(d[\"text\"] for d in docs)\n",
    "    def toks(s):\n",
    "        return set(re.findall(r\"[a-zA-ZÁÉÍÓÖŐÚÜŰáéíóöőúüű0-9]+\", s.lower()))\n",
    "    A = toks(answer)\n",
    "    C = toks(ctx)\n",
    "    if not A or not C:\n",
    "        return 0.0\n",
    "    overlap = len(A & C) / max(len(A), 1)\n",
    "    return min(1.0, overlap * 2.0)\n",
    "\n",
    "# --- State ---\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    question: str\n",
    "    plan: List[str] = field(default_factory=list)\n",
    "    step_idx: int = 0\n",
    "    gathered_docs: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    answer: str = \"\"\n",
    "    score: float = 0.0\n",
    "    iter: int = 0\n",
    "    max_iter: int = 2\n",
    "    logs: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "def log_event(state: AgentState, name: str, start_t: float, extra: Dict[str, Any] = None):\n",
    "    dt = time.time() - start_t\n",
    "    rec = {\"event\": name, \"elapsed_sec\": dt}\n",
    "    if extra:\n",
    "        rec.update(extra)\n",
    "    state.logs.append(rec)\n",
    "\n",
    "# --- Nodes ---\n",
    "def node_planner(state: AgentState) -> AgentState:\n",
    "    t0 = time.time()\n",
    "    state.plan = naive_plan(state.question)\n",
    "    state.step_idx = 0\n",
    "    log_event(state, \"planner\", t0, {\"plan_len\": len(state.plan)})\n",
    "    return state\n",
    "\n",
    "def node_retrieve(state: AgentState) -> AgentState:\n",
    "    t0 = time.time()\n",
    "    subq = state.plan[state.step_idx] if state.step_idx < len(state.plan) else state.question\n",
    "    docs = retrieve(subq, TOP_K)\n",
    "    log_event(state, \"retrieve\", t0, {\"hits\": len(docs)})\n",
    "    state.gathered_docs = docs\n",
    "    return state\n",
    "\n",
    "def node_rerank(state: AgentState) -> AgentState:\n",
    "    t0 = time.time()\n",
    "    reranked = RERANKER.rerank(state.question, state.gathered_docs, RERANK_TOP_K)\n",
    "    log_event(state, \"rerank\", t0, {\"kept\": len(reranked)})\n",
    "    state.gathered_docs = reranked\n",
    "    return state\n",
    "\n",
    "def node_synthesize(state: AgentState) -> AgentState:\n",
    "    t0 = time.time()\n",
    "    prompt = build_prompt(state.question, state.gathered_docs)\n",
    "    ans = LLM.generate(prompt, max_tokens=512)\n",
    "    state.answer = ans.strip()\n",
    "    log_event(state, \"synthesize\", t0, {\"answer_len\": len(state.answer)})\n",
    "    return state\n",
    "\n",
    "def node_critic(state: AgentState) -> AgentState:\n",
    "    t0 = time.time()\n",
    "    sc = simple_groundedness_score(state.answer, state.gathered_docs)\n",
    "    state.score = sc\n",
    "    log_event(state, \"critic\", t0, {\"score\": sc})\n",
    "    return state\n",
    "\n",
    "def should_refine(state: AgentState) -> str:\n",
    "    TH_OK = 0.25\n",
    "    if state.score >= TH_OK or state.iter >= state.max_iter:\n",
    "        return \"done\"\n",
    "    if len(state.gathered_docs) < max(1, RERANK_TOP_K // 2) and state.step_idx + 1 < len(state.plan):\n",
    "        state.step_idx += 1\n",
    "        state.iter += 1\n",
    "        return \"next_task\"\n",
    "    state.iter += 1\n",
    "    return \"refine\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c76e07",
   "metadata": {},
   "source": [
    "## 5) LangGraph: gráf felépítése és vezérlő logika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e7a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph összeállt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"planner\", node_planner)\n",
    "graph.add_node(\"retrieve\", node_retrieve)\n",
    "graph.add_node(\"rerank\", node_rerank)\n",
    "graph.add_node(\"synthesize\", node_synthesize)\n",
    "graph.add_node(\"critic\", node_critic)\n",
    "\n",
    "graph.set_entry_point(\"planner\")\n",
    "graph.add_edge(\"planner\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"rerank\")\n",
    "graph.add_edge(\"rerank\", \"synthesize\")\n",
    "graph.add_edge(\"synthesize\", \"critic\")\n",
    "\n",
    "def _route(state: AgentState):\n",
    "    decision = should_refine(state)\n",
    "    if decision == \"done\":\n",
    "        return END\n",
    "    elif decision == \"next_task\":\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "\n",
    "graph.add_conditional_edges(\"critic\", _route, {\n",
    "    \"retrieve\": \"retrieve\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "agent_app = graph.compile()\n",
    "print(\"LangGraph összeállt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa337f9",
   "metadata": {},
   "source": [
    "## 6) Példák és demó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791c2341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KÉRDÉS ===\n",
      "Foglalod össze a csatolt PDF(ek) fő témáit és kulcspontjait\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   27462.33 ms\n",
      "llama_perf_context_print: prompt eval time =   27461.43 ms /  1647 tokens (   16.67 ms per token,    59.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38204.05 ms /   511 runs   (   74.76 ms per token,    13.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   65826.04 ms /  2158 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n",
      "llama_perf_context_print: prompt eval time =   27461.43 ms /  1647 tokens (   16.67 ms per token,    59.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38204.05 ms /   511 runs   (   74.76 ms per token,    13.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   65826.04 ms /  2158 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n",
      "Llama.generate: 67 prefix-match hit, remaining 1344 prompt tokens to eval\n",
      "Llama.generate: 67 prefix-match hit, remaining 1344 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VÁLASZ ---\n",
      "1. A dokumentumokban szereplő tanulmányok a következő témákat érintik:\n",
      "   a) Neural machine translation (NMT) ([2], [3], [4], [23], [24])\n",
      "   b) Layer normalization ([1])\n",
      "   c) Dropout ([33])\n",
      "   d) Attention mechanisms ([4], [24], [25])\n",
      "   e) Deep learning architectures ([1], [3], [16], [20], [21])\n",
      "\n",
      "2. A dokumentumokban szereplő tanulmányok fontos kulcspontjai:\n",
      "   a) A Neural Turing Machines (NTMs) használata a NMT-ben ([4])\n",
      "   b) A LSTM-hálózatokban ([1], [16]) és a GRU-hálózatokban ([3]) alkalmazott layer normalization\n",
      "   c) A dropout technika megelőzése érdekében ([33])\n",
      "   d) A self-attentive sentence embedding ([22])\n",
      "   e) A positional embedding használata helyett a sinusoidal encoding ([4])\n",
      "   f) A nagy méretű neural networks ([32])\n",
      "   g) Adam optimizációs módszer ([20])\n",
      "   h) Factorization tricks for LSTM networks ([21])\n",
      "   i) A big-LSTM model ([11])\n",
      "\n",
      "3. A dokumentumokban szereplő különböző modellek perplexitásai és BLEU-szintjei:\n",
      "   a) A base model perplexitása: 4.92 ([4])\n",
      "   b) A (A) változat perplexitása: 5.29 ([1])\n",
      "   c) A (B) változat perplexitása: 5.16 ([16])\n",
      "   d) A (C) változat perplexitása: 6.11 ([3])\n",
      "   e) A (D) vál\n",
      "\n",
      "Score: 1.0\n",
      "Lépések: 5\n",
      "\n",
      "=== KÉRDÉS ===\n",
      "Mely részek indokolják a választott architektúrát, és milyen alternatívákat említenek?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   27462.33 ms\n",
      "llama_perf_context_print: prompt eval time =   21797.04 ms /  1344 tokens (   16.22 ms per token,    61.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20692.98 ms /   284 runs   (   72.86 ms per token,    13.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   42555.45 ms /  1628 tokens\n",
      "llama_perf_context_print:    graphs reused =        275\n",
      "llama_perf_context_print: prompt eval time =   21797.04 ms /  1344 tokens (   16.22 ms per token,    61.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20692.98 ms /   284 runs   (   72.86 ms per token,    13.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   42555.45 ms /  1628 tokens\n",
      "llama_perf_context_print:    graphs reused =        275\n",
      "Llama.generate: 67 prefix-match hit, remaining 1413 prompt tokens to eval\n",
      "Llama.generate: 67 prefix-match hit, remaining 1413 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VÁLASZ ---\n",
      "A választott architektúrákat a következő munkák indokolják:\n",
      "\n",
      "1. Attention mechanizmusok: [1], [16], [19]\n",
      "2. Neural GPU-k: [17]\n",
      "3. Linear időben történő neural machine translation: [18]\n",
      "4. Széles kapcsolatok: [2], [32]\n",
      "5. Szparse kapcsolatok: [20], [33]\n",
      "6. Memória hálózatok: [3], [34], [37]\n",
      "7. Nagy méretű neural networks: [35], [36]\n",
      "\n",
      "Alternatívák:\n",
      "\n",
      "1. Deep reinforced model for abstractive summarization: [28]\n",
      "2. Accurate, compact, és interpretable tree annotation: [29]\n",
      "3. Output embedding to improve language models: [30]\n",
      "4. Neural machine translation of rare words with subword units: [31]\n",
      "\n",
      "A fenti munkákban a szerzők a fent említett architektúrákat használták vagy alternatívákat javasoltak.\n",
      "\n",
      "Score: 1.0\n",
      "Lépések: 5\n",
      "\n",
      "=== KÉRDÉS ===\n",
      "Mik a rendszer jelenlegi korlátai és a jövőbeli fejlesztési lehetőségek?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   27462.33 ms\n",
      "llama_perf_context_print: prompt eval time =   23280.61 ms /  1413 tokens (   16.48 ms per token,    60.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22570.69 ms /   295 runs   (   76.51 ms per token,    13.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   45923.22 ms /  1708 tokens\n",
      "llama_perf_context_print:    graphs reused =        285\n",
      "llama_perf_context_print: prompt eval time =   23280.61 ms /  1413 tokens (   16.48 ms per token,    60.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22570.69 ms /   295 runs   (   76.51 ms per token,    13.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   45923.22 ms /  1708 tokens\n",
      "llama_perf_context_print:    graphs reused =        285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VÁLASZ ---\n",
      "A jelenlegi Transformer rendszer korlátai a következők:\n",
      "1. A képzési költség (FLOPs) és a méret (1018-1021) ([1]).\n",
      "2. A képzési algoritmus ([1]).\n",
      "3. A dropout alkalmazása a modellek alapján ([2]).\n",
      "\n",
      "A jövőbeli fejlesztési lehetőségek a következők:\n",
      "1. Növelhető a modellek mérete és a képzési költség ([1]).\n",
      "2. Továbbfejleszthető a modellek algoritmusa ([1]).\n",
      "3. Előrehaladóbb dropout és más optimális technikák alkalmazása a modellek alapján ([2]).\n",
      "4. További felhasználási területek, például multimodális adatfeldolgozás ([1]).\n",
      "5. Jobb integráció a felületes szintű szabályokkal, például a WSJ parsing esetén ([3]).\n",
      "\n",
      "Score: 0.542\n",
      "Lépések: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_agent(question: str, max_iter: int = 2) -> AgentState:\n",
    "    state = AgentState(question=question, max_iter=max_iter)\n",
    "    out = agent_app.invoke(state)\n",
    "    return out\n",
    "\n",
    "demo_questions = [\n",
    "    \"Foglalod össze a csatolt PDF(ek) fő témáit és kulcspontjait\",\n",
    "    \"Mely részek indokolják a választott architektúrát, és milyen alternatívákat említenek?\",\n",
    "    \"Mik a rendszer jelenlegi korlátai és a jövőbeli fejlesztési lehetőségek?\"\n",
    "]\n",
    "\n",
    "for q in demo_questions:\n",
    "    print(\"\\n=== KÉRDÉS ===\")\n",
    "    print(q)\n",
    "    res = run_agent(q, max_iter=2)\n",
    "    print(\"\\n--- VÁLASZ ---\")\n",
    "    print(res[\"answer\"])\n",
    "    print(\"\\nScore:\", round(res[\"score\"], 3))\n",
    "    print(\"Lépések:\", len(res[\"logs\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2e4e3",
   "metadata": {},
   "source": [
    "## 7) Teljesítménymérés és bottleneck elemzés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29682cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 67 prefix-match hit, remaining 1569 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   27462.33 ms\n",
      "llama_perf_context_print: prompt eval time =   26534.00 ms /  1569 tokens (   16.91 ms per token,    59.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37436.86 ms /   511 runs   (   73.26 ms per token,    13.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   64134.94 ms /  2080 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n",
      "llama_perf_context_print:        load time =   27462.33 ms\n",
      "llama_perf_context_print: prompt eval time =   26534.00 ms /  1569 tokens (   16.91 ms per token,    59.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37436.86 ms /   511 runs   (   73.26 ms per token,    13.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   64134.94 ms /  2080 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>elapsed_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synthesize</td>\n",
       "      <td>64.137620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rerank</td>\n",
       "      <td>0.096944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>retrieve</td>\n",
       "      <td>0.037660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>critic</td>\n",
       "      <td>0.000159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>planner</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        event  elapsed_sec\n",
       "4  synthesize    64.137620\n",
       "2      rerank     0.096944\n",
       "3    retrieve     0.037660\n",
       "0      critic     0.000159\n",
       "1     planner     0.000008"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def summarize_logs(logs: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    if not logs:\n",
    "        return pd.DataFrame(columns=[\"event\", \"elapsed_sec\"])\n",
    "    df = pd.DataFrame(logs)\n",
    "    agg = df.groupby(\"event\")[\"elapsed_sec\"].sum().reset_index().sort_values(\"elapsed_sec\", ascending=False)\n",
    "    return agg\n",
    "\n",
    "def plot_bottlenecks(df: pd.DataFrame, title: str = \"Bottleneck-ek\"):\n",
    "    if df.empty:\n",
    "        print(\"Nincs adat a grafikonhoz.\")\n",
    "        return\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar(df[\"event\"], df[\"elapsed_sec\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Komponens\")\n",
    "    plt.ylabel(\"Össz. idő [s]\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sample_q = \"Kérlek adj összefoglalót a dokumentumokról, kiemelve a fő fejezeteket.\"\n",
    "res = run_agent(sample_q, max_iter=2)\n",
    "df_logs = summarize_logs(res[\"logs\"])\n",
    "df_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbaa408a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_bottlenecks(df_logs, title='Agent futás - komponens idők')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f08285",
   "metadata": {},
   "source": [
    "## 8) Gyors tesztek (smoke tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3b9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 67 prefix-match hit, remaining 1579 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   27462.33 ms\n",
      "llama_perf_context_print: prompt eval time =   26453.95 ms /  1579 tokens (   16.75 ms per token,    59.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37617.88 ms /   511 runs   (   73.62 ms per token,    13.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   64234.22 ms /  2090 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n",
      "llama_perf_context_print:        load time =   27462.33 ms\n",
      "llama_perf_context_print: prompt eval time =   26453.95 ms /  1579 tokens (   16.75 ms per token,    59.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37617.88 ms /   511 runs   (   73.62 ms per token,    13.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   64234.22 ms /  2090 tokens\n",
      "llama_perf_context_print:    graphs reused =        494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] FAISS típus\n",
      "[OK] Planner plan elemszám > 0\n",
      "[OK] Retrieve lefut\n",
      "[OK] Agent válasz típusa\n",
      "[OK] Critic score tartomány\n",
      "Összegzés: 5/5 sikeres teszt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def smoke_tests():\n",
    "    tests = []\n",
    "    # 1) Index típus\n",
    "    tests.append((\"FAISS típus\", hasattr(__import__('faiss'), 'IndexFlatIP')))\n",
    "    # 2) Planner\n",
    "    plan = naive_plan(\"Adj összefoglalót és hasonlítsd össze a megközelítéseket, majd javasolj fejlesztést.\")\n",
    "    tests.append((\"Planner plan elemszám > 0\", len(plan) > 0))\n",
    "    # 3) Retrieve lefut\n",
    "    r = retrieve(\"teszt kérdés\", top_k=3)\n",
    "    tests.append((\"Retrieve lefut\", isinstance(r, list)))\n",
    "    # 4) Agent válasz\n",
    "    res = run_agent(\"Mi a dokumentumok fő tartalma?\", max_iter=1)\n",
    "    tests.append((\"Agent válasz típusa\", isinstance(res[\"answer\"], str)))\n",
    "    # 5) Critic score 0..1\n",
    "    tests.append((\"Critic score tartomány\", 0.0 <= res[\"score\"] <= 1.0))\n",
    "\n",
    "    ok = sum(1 for _, t in tests if t)\n",
    "    for name, passed in tests:\n",
    "        print(f\"[{'OK' if passed else 'FAIL'}] {name}\")\n",
    "    print(f\"Összegzés: {ok}/{len(tests)} sikeres teszt.\")\n",
    "    return all(t for _, t in tests)\n",
    "\n",
    "_ = smoke_tests()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
